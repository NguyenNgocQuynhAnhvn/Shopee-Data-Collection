{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helpers.py\n",
    "import pickle\n",
    "from time import sleep\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "def scroll_down(driver, time_sleep=1):\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "    while True:\n",
    "        # Cuộn xuống dưới cùng của trang\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        sleep(time_sleep)\n",
    "\n",
    "        # Chờ trang tải thêm nội dung\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        \n",
    "        # Nếu chiều cao trang không đổi, tức là đã tải hết nội dung\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        last_height = new_height  # Cập nhật chiều cao trang mới\n",
    "\n",
    "\n",
    "def save_cookie(driver, cookie_name):\n",
    "    with open(cookie_name, 'wb') as file:\n",
    "        pickle.dump(driver.get_cookies(), file)\n",
    "\n",
    "def load_cookie(driver, cookie_name):\n",
    "    with open(cookie_name, 'rb') as file:\n",
    "        for cookie in pickle.load(file):\n",
    "            driver.add_cookie(cookie)\n",
    "\n",
    "def log_error(text, filename=\"log_error.txt\"):\n",
    "    with open(filename, \"a\", encoding='utf-8') as f:\n",
    "        f.write(text + \"\\n\")\n",
    "\n",
    "def get_data(soup, selector):\n",
    "    try:\n",
    "        return soup.select(selector)[0].text\n",
    "    except IndexError:\n",
    "        return None\n",
    "\n",
    "def info_shop(soup):\n",
    "    res = {'Đánh giá': None, 'tỉ lệ phản hồi': None, 'tham gia': None, 'Sản phẩm': None, 'thời gian phản hồi': None, 'Người theo dõi': None}\n",
    "    res['shop_name'] = get_data(soup, \".fV3TIn\")\n",
    "    brand = get_data(soup, \".ZUZ1FO\") or ('mall' if get_data(soup, \".official-shop-new-badge\") == '' else 'normal')\n",
    "    res['shop_brand'] = brand\n",
    "    for i in soup.select('.NGzCXN > .YnZi6x'):\n",
    "        try:\n",
    "            res[i.label.text] = i.span.text\n",
    "        except AttributeError:\n",
    "            pass\n",
    "    return res\n",
    "\n",
    "def extract_feedback(soup):\n",
    "    feedback_count = 0\n",
    "    feedbacks = []\n",
    "    feedback_list = soup.find_all('div', class_='shopee-product-rating')\n",
    "    for feedback in feedback_list:\n",
    "        user_name = feedback.find('a').text if feedback.find('a') else \"Ẩn danh\"\n",
    "        star_rating = len(feedback.find_all('svg', class_='icon-rating-solid--active'))\n",
    "        comment_div = feedback.find('div', style=lambda v: v and 'color: rgba(0, 0, 0, 0.87)' in v)\n",
    "        comment = \" \".join(comment_div.stripped_strings) if comment_div else None\n",
    "        feedbacks.append({'user_name': user_name, 'star_rating': star_rating, 'comment': comment})\n",
    "    \n",
    "    \n",
    "    return feedbacks\n",
    "def scrape_all_feedback(driver, max_pages=10):\n",
    "    all_feedbacks = []\n",
    "    page_count = 0  # Biến đếm trang\n",
    "\n",
    "    while page_count < max_pages:\n",
    "        # Lấy mã nguồn của trang hiện tại\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        \n",
    "        # Gọi hàm extract_feedback để lấy feedback từ trang hiện tại\n",
    "        feedbacks = extract_feedback(soup)\n",
    "        all_feedbacks.extend(feedbacks)\n",
    "\n",
    "        try:\n",
    "            # Tìm và click vào nút \"next page\"\n",
    "            next_button = driver.find_element(By.XPATH, \"//button[contains(@class, 'shopee-icon-button shopee-icon-button--right ')]\")\n",
    "            next_button.click()\n",
    "            sleep(2)  # Đợi một chút để trang tải xong\n",
    "            \n",
    "            page_count += 1  # Tăng biến đếm trang\n",
    "        except NoSuchElementException:\n",
    "            # Nếu không tìm thấy nút \"next page\", thoát khỏi vòng lặp\n",
    "            break\n",
    "\n",
    "    return all_feedbacks\n",
    "\n",
    "def extract_data(driver, soup):\n",
    "    res = {\n",
    "        'name_product': get_data(soup, \".WBVL_7 > span\"),\n",
    "        'price_origin': get_data(soup, \".qg2n76\"),\n",
    "        'price': get_data(soup, \".G27FPf\"),\n",
    "        'rate': get_data(soup, \".dQEiAI\"),\n",
    "        'review_count': get_data(soup, \".e2p50f > div[class='F9RHbS']\"),\n",
    "        'sale_count': get_data(soup, \".AcmPRb\"),\n",
    "        'like_count': get_data(soup, \".w2JMKY > .rhG6k7\"),\n",
    "        'fee_delivery': get_data(soup, \".PZGOkt\"),\n",
    "        'is_flash_sale': 1 if get_data(soup, \".x7M8PV\") else 0,\n",
    "        'quantity': get_data(soup, \".OaFP0p > div > div:nth-child(2)\"),\n",
    "        'descibe': get_data(soup, \".e8lZp3\"),\n",
    "        'detail': get_data(soup, \".Gf4Ro0\")\n",
    "    }\n",
    "    shop = info_shop(soup.find(class_=\"page-product__shop\"))\n",
    "    res.update(shop)\n",
    "    res['feedbacks'] = scrape_all_feedback(driver, max_pages=10)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'config' from 'config' (c:\\Users\\quynh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\config\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mselenium\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwebdriver\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchrome\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Options\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhelpers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m scroll_down, save_cookie, load_cookie, log_error, extract_data\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sleep\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'config' from 'config' (c:\\Users\\quynh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\config\\__init__.py)"
     ]
    }
   ],
   "source": [
    "# scraper.py\n",
    "import undetected_chromedriver as uc\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from tqdm import tqdm\n",
    "from config import config\n",
    "from helpers import scroll_down, save_cookie, load_cookie, log_error, extract_data\n",
    "from time import sleep\n",
    "from bs4 import BeautifulSoup\n",
    "from pymongo import MongoClient\n",
    "\n",
    "# hàm setup mongodb\n",
    "def setup_mongo():\n",
    "    client = MongoClient(\"mongodb://localhost:27017/\")\n",
    "    db = client[\"Shopee_data\"]\n",
    "    return db[\"Products\"]\n",
    "\n",
    "#  thiết lập Selenium driver\n",
    "def setup_driver():\n",
    "    chrome_options = Options()\n",
    "    chrome_options.headless = False # Tắt chế độ headless (hiển thị trình duyệt)\n",
    "    chrome_options.add_argument('disable-infobars')  # Tắt thanh thông tin\n",
    "    chrome_options.add_argument('--disable-extensions') # Tắt các tiện ích mở rộng\n",
    "    chrome_options.add_argument('--no-sandbox') # Tắt chế độ sandbox\n",
    "    driver = uc.Chrome(options=chrome_options) # Khởi tạo trình điều khiển Chrome với các tùy chọn đã thiết lập\n",
    "    driver.maximize_window()\n",
    "    return driver\n",
    "\n",
    "# Trích xuất dữ liệu từ một trang cụ thể và lưu dữ liệu vào MongoDB.\n",
    "def scrape_page(driver, url_page, collection):\n",
    "    driver.get(url_page)\n",
    "    sleep(10)\n",
    "    scroll_down(driver, 1.0)\n",
    "    products = driver.find_elements(By.XPATH, '//li[@data-sqe=\"item\"]')\n",
    "    print(f\"Tìm thấy {len(products)} sản phẩm trên {url_page}\")\n",
    "    \n",
    "    if not products:\n",
    "        print(\"Không tìm thấy sản phẩm nào!\")\n",
    "        return\n",
    "\n",
    "    links = []\n",
    "    for box in products:\n",
    "        try:\n",
    "            link_product = box.find_element(By.XPATH, './/a[@class=\"contents\"]').get_attribute('href')\n",
    "            link_image = box.find_element(By.XPATH, './/div[@class=\"relative z-0 w-full pt-full\"]//img').get_attribute('src')\n",
    "            links.append((link_product, link_image))\n",
    "        except Exception as e:\n",
    "            print(f\"Lỗi khi lấy link từ một sản phẩm: {e}\")\n",
    "            continue\n",
    "\n",
    "    for link, img in tqdm(links, desc=\"Scraping products\", ncols=100, colour='green'):\n",
    "        driver.get(link)\n",
    "        sleep(8)\n",
    "        scroll_down(driver, 1.0)\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        data = extract_data(driver, soup)\n",
    "        data.update({\"link_image\": img, \"link_product\": link})\n",
    "        collection.insert_one(data)  # Insert data into MongoDB collection\n",
    "\n",
    "# Hàm chính để trích xuất dữ liệu từ nhiều trang và lưu vào MongoDB.\n",
    "def main():\n",
    "    driver = setup_driver()\n",
    "    driver.get(config['base_url'])\n",
    "    load_cookie(driver, 'cookies.pkl')\n",
    "    driver.refresh()\n",
    "\n",
    "    collection = setup_mongo()  # Set up MongoDB collection\n",
    "    for page in range(10):\n",
    "        log_error(f'# Page {page}')\n",
    "        url_page = f\"{config['base_url']}/{config['brand_url']}?page={page}\"\n",
    "        scrape_page(driver, url_page, collection)\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
