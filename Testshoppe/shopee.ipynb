{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helpers.py\n",
    "import pickle\n",
    "from time import sleep\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.common.exceptions import NoSuchElementException # type: ignore\n",
    "from selenium.webdriver.common.by import By # type: ignore\n",
    "\n",
    "\n",
    "def scroll_down(driver, time_sleep=1):\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "    while True:\n",
    "        # Cuộn xuống dưới cùng của trang\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        sleep(time_sleep)\n",
    "\n",
    "        # Chờ trang tải thêm nội dung\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        \n",
    "        # Nếu chiều cao trang không đổi, tức là đã tải hết nội dung\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        last_height = new_height  # Cập nhật chiều cao trang mới\n",
    "\n",
    "\n",
    "def save_cookie(driver, cookie_name):\n",
    "    with open(cookie_name, 'wb') as file:\n",
    "        pickle.dump(driver.get_cookies(), file)\n",
    "\n",
    "def load_cookie(driver, cookie_name):\n",
    "    with open(cookie_name, 'rb') as file:\n",
    "        for cookie in pickle.load(file):\n",
    "            driver.add_cookie(cookie)\n",
    "\n",
    "def log_error(text, filename=\"log_error.txt\"):\n",
    "    with open(filename, \"a\", encoding='utf-8') as f:\n",
    "        f.write(text + \"\\n\")\n",
    "\n",
    "def get_data(soup, selector):\n",
    "    try:\n",
    "        return soup.select(selector)[0].text\n",
    "    except IndexError:\n",
    "        return None\n",
    "\n",
    "def info_shop(soup):\n",
    "    res = {'Đánh giá': None, 'tỉ lệ phản hồi': None, 'tham gia': None, 'Sản phẩm': None, 'thời gian phản hồi': None, 'Người theo dõi': None}\n",
    "    res['shop_name'] = get_data(soup, \".fV3TIn\")\n",
    "    brand = get_data(soup, \".ZUZ1FO\") or ('mall' if get_data(soup, \".official-shop-new-badge\") == '' else 'normal')\n",
    "    res['shop_brand'] = brand\n",
    "    for i in soup.select('.NGzCXN > .YnZi6x'):\n",
    "        try:\n",
    "            res[i.label.text] = i.span.text\n",
    "        except AttributeError:\n",
    "            pass\n",
    "    return res\n",
    "\n",
    "def extract_feedback(soup):\n",
    "    feedback_count = 0\n",
    "    feedbacks = []\n",
    "    feedback_list = soup.find_all('div', class_='shopee-product-rating')\n",
    "    for feedback in feedback_list:\n",
    "        user_name = feedback.find('a').text if feedback.find('a') else \"Ẩn danh\"\n",
    "        star_rating = len(feedback.find_all('svg', class_='icon-rating-solid--active'))\n",
    "        comment_div = feedback.find('div', style=lambda v: v and 'color: rgba(0, 0, 0, 0.87)' in v)\n",
    "        comment = \" \".join(comment_div.stripped_strings) if comment_div else None\n",
    "        feedbacks.append({'user_name': user_name, 'star_rating': star_rating, 'comment': comment})\n",
    "    \n",
    "    \n",
    "    return feedbacks\n",
    "def scrape_all_feedback(driver, max_pages=10):\n",
    "    all_feedbacks = []\n",
    "    page_count = 0  # Biến đếm trang\n",
    "\n",
    "    while page_count < max_pages:\n",
    "        # Lấy mã nguồn của trang hiện tại\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        \n",
    "        # Gọi hàm extract_feedback để lấy feedback từ trang hiện tại\n",
    "        feedbacks = extract_feedback(soup)\n",
    "        all_feedbacks.extend(feedbacks)\n",
    "\n",
    "        try:\n",
    "            # Tìm và click vào nút \"next page\"\n",
    "            next_button = driver.find_element(By.XPATH, \"//button[contains(@class, 'shopee-icon-button shopee-icon-button--right ')]\")\n",
    "            next_button.click()\n",
    "            sleep(2)  # Đợi một chút để trang tải xong\n",
    "            \n",
    "            page_count += 1  # Tăng biến đếm trang\n",
    "        except NoSuchElementException:\n",
    "            # Nếu không tìm thấy nút \"next page\", thoát khỏi vòng lặp\n",
    "            break\n",
    "\n",
    "    return all_feedbacks\n",
    "\n",
    "def extract_data(driver, soup):\n",
    "    res = {\n",
    "        'name_product': get_data(soup, \".WBVL_7 > span\"),\n",
    "        'price_origin': get_data(soup, \".qg2n76\"),\n",
    "        'price': get_data(soup, \".G27FPf\"),\n",
    "        'rate': get_data(soup, \".dQEiAI\"),\n",
    "        'review_count': get_data(soup, \".e2p50f > div[class='F9RHbS']\"),\n",
    "        'sale_count': get_data(soup, \".AcmPRb\"),\n",
    "        'like_count': get_data(soup, \".w2JMKY > .rhG6k7\"),\n",
    "        'fee_delivery': get_data(soup, \".PZGOkt\"),\n",
    "        'is_flash_sale': 1 if get_data(soup, \".x7M8PV\") else 0,\n",
    "        'quantity': get_data(soup, \".OaFP0p > div > div:nth-child(2)\"),\n",
    "        'descibe': get_data(soup, \".e8lZp3\"),\n",
    "        'detail': get_data(soup, \".Gf4Ro0\")\n",
    "    }\n",
    "    shop = info_shop(soup.find(class_=\"page-product__shop\"))\n",
    "    res.update(shop)\n",
    "    res['feedbacks'] = scrape_all_feedback(driver, max_pages=10)\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scraper.py\n",
    "import undetected_chromedriver as uc \n",
    "from selenium.webdriver.common.by import By \n",
    "from selenium.webdriver.chrome.options import Options \n",
    "from tqdm import tqdm \n",
    "import pandas as pd \n",
    "from config import config \n",
    "from helpers import scroll_down, save_cookie, load_cookie, log_error, extract_data, extract_feedback # type: ignore\n",
    "from time import sleep\n",
    "from bs4 import BeautifulSoup \n",
    "\n",
    "def setup_driver():\n",
    "    chrome_options = Options()\n",
    "    chrome_options.headless = False\n",
    "    chrome_options.add_argument('disable-infobars')\n",
    "    chrome_options.add_argument('--disable-extensions')\n",
    "    chrome_options.add_argument('--no-sandbox')\n",
    "    driver = uc.Chrome(chrome_options=chrome_options)\n",
    "    driver.maximize_window()\n",
    "\n",
    "    return driver\n",
    "\n",
    "\n",
    "def scrape_page(driver, url_page, save_file):\n",
    "    driver.get(url_page)\n",
    "    sleep(10)\n",
    "    scroll_down(driver, 1.0)\n",
    "    # Lấy tất cả box sản phẩm\n",
    "    products = driver.find_elements(By.XPATH, '//li[@data-sqe=\"item\"]')\n",
    "    print(f\"Tìm thấy {len(products)} sản phẩm trên {url_page}\")\n",
    "    \n",
    "    # Nếu không có sản phẩm, kết thúc hàm\n",
    "    if not products:\n",
    "        print(\"Không tìm thấy sản phẩm nào!\")\n",
    "        return\n",
    "\n",
    "    # Lấy link sản phẩm và link ảnh\n",
    "    links = []\n",
    "    for box in products:\n",
    "        try:\n",
    "            # Lấy URL của sản phẩm và ảnh\n",
    "            link_product = box.find_element(By.XPATH, './/a[@class=\"contents\"]').get_attribute('href')\n",
    "            link_image = box.find_element(By.XPATH, './/div[@class=\"relative z-0 w-full pt-full\"]//img').get_attribute('src')\n",
    "            links.append((link_product, link_image))\n",
    "        except Exception as e:\n",
    "            print(f\"Lỗi khi lấy link từ một sản phẩm: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Trích xuất dữ liệu từ từng sản phẩm\n",
    "    for link, img in tqdm(links, desc=\"Scraping products\", ncols=100, colour='green'):\n",
    "        driver.get(link)\n",
    "        sleep(8)\n",
    "        scroll_down(driver, 0.8)\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        # Lấy dữ liệu chi tiết sản phẩm\n",
    "        data = extract_data(driver, soup)\n",
    "        data.update({\"link_image\": img, \"link_product\": link})\n",
    "        # Lưu dữ liệu vào file CSV\n",
    "        pd.DataFrame([data]).to_csv(save_file, mode='a', header=not pd.io.common.file_exists(save_file), index=False)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    driver = setup_driver()\n",
    "    driver.get(config['base_url'])\n",
    "    load_cookie(driver, 'cookies.pkl')\n",
    "    driver.refresh()\n",
    "\n",
    "    save_file = 'shopee.csv'\n",
    "    for page in range(10):\n",
    "        log_error(f'# Page {page}')\n",
    "        url_page = f\"{config['base_url']}/{config['brand_url']}?page={page}\"\n",
    "        scrape_page(driver, url_page, save_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import undetected_chromedriver as uc\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from config import config\n",
    "from helpers import scroll_down, save_cookie, load_cookie, log_error, extract_data\n",
    "from time import sleep\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def setup_driver():\n",
    "    chrome_options = Options()\n",
    "    chrome_options.headless = False  # Đảm bảo hiển thị trình duyệt\n",
    "    chrome_options.add_argument('disable-infobars')\n",
    "    chrome_options.add_argument('--disable-extensions')\n",
    "    chrome_options.add_argument('--no-sandbox')\n",
    "    driver = uc.Chrome(options=chrome_options)\n",
    "    driver.maximize_window()\n",
    "    return driver\n",
    "\n",
    "def scrape_page(driver, url_page, save_file):\n",
    "    driver.get(url_page)\n",
    "    sleep(30)\n",
    "    scroll_down(driver, 1.0)\n",
    "    products = driver.find_elements(By.XPATH, '//li[@data-sqe=\"item\"]')\n",
    "    print(f\"Tìm thấy {len(products)} sản phẩm trên {url_page}\")\n",
    "\n",
    "    if not products:\n",
    "        print(\"Không tìm thấy sản phẩm nào!\")\n",
    "        return\n",
    "\n",
    "    links = []\n",
    "    for box in products:\n",
    "        try:\n",
    "            link_product = box.find_element(By.XPATH, './/a[@class=\"contents\"]').get_attribute('href')\n",
    "            link_image = box.find_element(By.XPATH, './/div[@class=\"relative z-0 w-full pt-full\"]//img').get_attribute('src')\n",
    "            links.append((link_product, link_image))\n",
    "        except Exception as e:\n",
    "            print(f\"Lỗi khi lấy link từ một sản phẩm: {e}\")\n",
    "            continue\n",
    "\n",
    "    for link, img in tqdm(links, desc=\"Scraping products\", ncols=100, colour='green'):\n",
    "        driver.get(link)\n",
    "        sleep(8)\n",
    "        scroll_down(driver, 1.0)\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        data = extract_data(driver, soup)\n",
    "        data.update({\"link_image\": img, \"link_product\": link})\n",
    "        pd.DataFrame([data]).to_csv(save_file, mode='a', header=not pd.io.common.file_exists(save_file), index=False)\n",
    "\n",
    "def process_pages(pages, save_file):\n",
    "    driver = setup_driver()\n",
    "    driver.get(config['base_url'])\n",
    "    load_cookie(driver, 'cookies.pkl')\n",
    "    driver.refresh()\n",
    "    for page in pages:\n",
    "        log_error(f'# Page {page}')\n",
    "        url_page = f\"{config['base_url']}/{config['brand_url']}?page={page}\"\n",
    "        scrape_page(driver, url_page, save_file)\n",
    "    driver.quit()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    save_file = 'shopee.csv'\n",
    "    pages = list(range(10))  # Danh sách từ 0 đến 9 (10 trang)\n",
    "    page_pairs = [pages[i:i+2] for i in range(0, len(pages), 2)]\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=2) as executor:\n",
    "        futures = [executor.submit(process_pages, page_pair, save_file) for page_pair in page_pairs]\n",
    "\n",
    "        for future in futures:\n",
    "            future.result()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
